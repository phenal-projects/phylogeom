{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('../src/') # for graph_data\n",
    "\n",
    "import graph_data as gd\n",
    "import time\n",
    "import torch\n",
    "from models import TreeSupport\n",
    "from torch import optim\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from Bio import Phylo as phy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.enable('data_server')\n",
    "alt.renderers.enable('mimetype')\n",
    "\n",
    "torch.manual_seed(245)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 1,
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2b66aed750>"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T09:40:40.195Z",
     "iopub.execute_input": "2020-04-16T09:40:40.207Z",
     "shell.execute_reply": "2020-04-16T09:40:42.084Z",
     "iopub.status.idle": "2020-04-16T09:40:42.053Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datasets\n",
    "## Prepare trees and reconstructed sequences"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "data_path = '../data/'"
   ],
   "outputs": [],
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T09:40:42.793Z",
     "iopub.execute_input": "2020-04-16T09:40:42.801Z",
     "iopub.status.idle": "2020-04-16T09:40:42.814Z",
     "shell.execute_reply": "2020-04-16T09:40:42.821Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "target_tree = phy.read(data_path+'tree/Fungi45_infer/Fungi.tre', 'newick')\n",
    "#all_data = gd.Trees.load_ready_trees(data_path+'tree/Fungi45_infer/fml_output/', data_path+'tree/Fungi45_infer/fml_output/*.tre', target_tree)  # INITIALIZE NEW DS\n",
    "fungi_data = gd.Trees(data_path+'tree/Fungi45_infer/', data_path+'alns/Fungi45/', target_tree)\n",
    "fungi_data.data.x = fungi_data.data.x.float()\n",
    "fungi_data.data.edge_attr = (torch.max(fungi_data.data.edge_attr)+0.001 - fungi_data.data.edge_attr).float()\n",
    "fungi_data.data.edge_index = fungi_data.data.edge_index.long()"
   ],
   "outputs": [],
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "execution": {
     "iopub.status.busy": "2020-04-16T09:40:43.603Z",
     "iopub.execute_input": "2020-04-16T09:40:43.611Z",
     "iopub.status.idle": "2020-04-16T09:40:43.793Z",
     "shell.execute_reply": "2020-04-16T09:40:43.807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "target_tree = phy.read(data_path+'tree/Archaea/Archaea.tre', 'newick')\n",
    "#archaea_data = gd.Trees.load_ready_trees(data_path+'tree/Archaea/fml_output/', data_path+'tree/Archaea/fml_output/*.tre', target_tree)  # INITIALIZE NEW DS //delete temp_tree\n",
    "archaea_data = gd.Trees(data_path+\"tree/Archaea/\", data_path+\"alns/Archaea/\", target_tree)\n",
    "archaea_data.data.x = archaea_data.data.x.float()\n",
    "archaea_data.data.edge_attr = (torch.max(archaea_data.data.edge_attr)+0.001 - archaea_data.data.edge_attr).float()\n",
    "archaea_data.data.edge_index = archaea_data.data.edge_index.long()"
   ],
   "outputs": [],
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T09:40:44.031Z",
     "iopub.execute_input": "2020-04-16T09:40:44.039Z",
     "iopub.status.idle": "2020-04-16T09:40:44.102Z",
     "shell.execute_reply": "2020-04-16T09:40:44.111Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# concatenate datasets\n",
    "all_data =  ConcatDataset([fungi_data, archaea_data])"
   ],
   "outputs": [],
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T09:40:44.505Z",
     "iopub.execute_input": "2020-04-16T09:40:44.525Z",
     "iopub.status.idle": "2020-04-16T09:40:44.550Z",
     "shell.execute_reply": "2020-04-16T09:40:44.560Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# data preparation\n",
    "train_batch = 50\n",
    "test_batch = 50\n",
    "# data loaders\n",
    "train, test = random_split(\n",
    "    all_data, [round(len(all_data) * 0.8), round(len(all_data) * 0.2)]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T09:40:45.070Z",
     "iopub.execute_input": "2020-04-16T09:40:45.077Z",
     "iopub.status.idle": "2020-04-16T09:40:45.088Z",
     "shell.execute_reply": "2020-04-16T09:40:45.096Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lst = np.zeros(len(train))\n",
    "for i, data in enumerate(train):\n",
    "    lst[i] = -(torch.sum(torch.log2(data.y.squeeze()))/len(data.y)).item()\n",
    "lst = pd.DataFrame(lst, columns=['Q'])\n",
    "alt.Chart(lst).mark_bar().encode(\n",
    "    alt.X(\"Q:Q\", bin=alt.Bin(maxbins=100)),\n",
    "    y='count()',\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 7,
     "data": {
      "application/vnd.vegalite.v4+json": {
       "config": {
        "view": {
         "continuousWidth": 400,
         "continuousHeight": 300
        }
       },
       "data": {
        "url": "http://localhost:15247/40227422fdf3a2297bfb53565c1060f9.json"
       },
       "mark": "bar",
       "encoding": {
        "x": {
         "type": "quantitative",
         "bin": {
          "maxbins": 100
         },
         "field": "Q"
        },
        "y": {
         "type": "quantitative",
         "aggregate": "count"
        }
       },
       "$schema": "https://vega.github.io/schema/vega-lite/v4.0.2.json"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "execution": {
     "iopub.status.busy": "2020-04-16T09:40:45.727Z",
     "iopub.execute_input": "2020-04-16T09:40:45.735Z",
     "iopub.status.idle": "2020-04-16T09:40:46.150Z",
     "shell.execute_reply": "2020-04-16T09:40:46.199Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train_dl = DataLoader(\n",
    "    train, batch_size=train_batch, pin_memory=True, num_workers=3\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test, batch_size=test_batch, num_workers=2\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T09:40:46.916Z",
     "iopub.execute_input": "2020-04-16T09:40:46.923Z",
     "iopub.status.idle": "2020-04-16T09:40:46.934Z",
     "shell.execute_reply": "2020-04-16T09:40:46.942Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = TreeSupport(231, 400)\n",
    "model = model.cuda()\n",
    "silent = False\n",
    "num_epochs = 200\n",
    "loss_fn = MSELoss()\n",
    "test_batches = len(test_dl)\n",
    "# stattr\n",
    "losses = []\n",
    "val_losses = []\n",
    "learning_rates = []"
   ],
   "outputs": [],
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T09:40:47.359Z",
     "iopub.execute_input": "2020-04-16T09:40:47.365Z",
     "shell.execute_reply": "2020-04-16T09:40:49.297Z",
     "iopub.status.idle": "2020-04-16T09:40:49.282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# interactive descending lr for less loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.5, verbose=True, cooldown=2, patience=5\n",
    ")\n",
    "num_epochs = 600\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data in train_dl:\n",
    "        data = data.to(torch.device('cuda'))\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_fn(out, data.y)\n",
    "        losses.append(loss.item())  # history\n",
    "        # optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # stats\n",
    "        learning_rates.append(-1)\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()\n",
    "    agg_loss = 0\n",
    "    with torch.autograd.no_grad():\n",
    "        for data in test_dl:\n",
    "            data = data.to(torch.device('cuda'))\n",
    "            out = model(data)\n",
    "            agg_loss += loss_fn(out, data.y)\n",
    "        val_losses.append(agg_loss.item()/test_batches)\n",
    "    scheduler.step(agg_loss)\n",
    "    if not silent:\n",
    "        print(\n",
    "            \"Epoch [{}/{}], Loss (last training batch/val): {:.4f}/{:.4f}. Time elapsed: {:.2f}\".format(\n",
    "                epoch + 1,\n",
    "                num_epochs,\n",
    "                losses[-1],\n",
    "                val_losses[-1],\n",
    "                time.time() - start,\n",
    "            )\n",
    "        )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/600], Loss (last training batch/val): 0.2635/0.2633. Time elapsed: 2.58\n",
      "Epoch [2/600], Loss (last training batch/val): 0.2527/0.2518. Time elapsed: 4.77\n",
      "Epoch [3/600], Loss (last training batch/val): 0.2472/0.2466. Time elapsed: 6.94\n",
      "Epoch [4/600], Loss (last training batch/val): 0.2413/0.2415. Time elapsed: 9.36\n",
      "Epoch [5/600], Loss (last training batch/val): 0.2353/0.2359. Time elapsed: 11.54\n",
      "Epoch [6/600], Loss (last training batch/val): 0.2297/0.2305. Time elapsed: 13.76\n",
      "Epoch [7/600], Loss (last training batch/val): 0.2259/0.2270. Time elapsed: 15.98\n",
      "Epoch [8/600], Loss (last training batch/val): 0.2203/0.2218. Time elapsed: 18.19\n",
      "Epoch [9/600], Loss (last training batch/val): 0.2141/0.2152. Time elapsed: 20.38\n",
      "Epoch [10/600], Loss (last training batch/val): 0.2090/0.2098. Time elapsed: 22.57\n",
      "Epoch [11/600], Loss (last training batch/val): 0.2030/0.2042. Time elapsed: 24.75\n",
      "Epoch [12/600], Loss (last training batch/val): 0.2172/0.2055. Time elapsed: 26.95\n",
      "Epoch [13/600], Loss (last training batch/val): 0.1938/0.1978. Time elapsed: 29.14\n",
      "Epoch [14/600], Loss (last training batch/val): 0.1967/0.1905. Time elapsed: 31.38\n",
      "Epoch [15/600], Loss (last training batch/val): 0.1948/0.1854. Time elapsed: 33.77\n",
      "Epoch [16/600], Loss (last training batch/val): 0.1905/0.1803. Time elapsed: 35.95\n",
      "Epoch [17/600], Loss (last training batch/val): 0.1853/0.1764. Time elapsed: 38.14\n",
      "Epoch [18/600], Loss (last training batch/val): 0.1750/0.1715. Time elapsed: 40.36\n",
      "Epoch [19/600], Loss (last training batch/val): 0.1725/0.1677. Time elapsed: 42.55\n",
      "Epoch [20/600], Loss (last training batch/val): 0.1676/0.1637. Time elapsed: 44.77\n",
      "Epoch [21/600], Loss (last training batch/val): 0.1629/0.1599. Time elapsed: 47.19\n",
      "Epoch [22/600], Loss (last training batch/val): 0.1679/0.1805. Time elapsed: 49.38\n",
      "Epoch [23/600], Loss (last training batch/val): 0.1560/0.1538. Time elapsed: 51.58\n",
      "Epoch [24/600], Loss (last training batch/val): 0.1667/0.1926. Time elapsed: 53.76\n",
      "Epoch [25/600], Loss (last training batch/val): 0.1615/0.1749. Time elapsed: 55.96\n",
      "Epoch [26/600], Loss (last training batch/val): 0.1455/0.1440. Time elapsed: 58.16\n",
      "Epoch [27/600], Loss (last training batch/val): 0.1600/0.1894. Time elapsed: 60.35\n",
      "Epoch [28/600], Loss (last training batch/val): 0.1402/0.1393. Time elapsed: 62.53\n",
      "Epoch [29/600], Loss (last training batch/val): 0.1424/0.1581. Time elapsed: 64.76\n",
      "Epoch [30/600], Loss (last training batch/val): 0.1417/0.1597. Time elapsed: 66.96\n",
      "Epoch [31/600], Loss (last training batch/val): 0.1432/0.1616. Time elapsed: 69.17\n",
      "Epoch [32/600], Loss (last training batch/val): 0.1376/0.1502. Time elapsed: 71.34\n",
      "Epoch [33/600], Loss (last training batch/val): 0.1302/0.1279. Time elapsed: 73.53\n",
      "Epoch [34/600], Loss (last training batch/val): 0.1282/0.1267. Time elapsed: 75.73\n",
      "Epoch [35/600], Loss (last training batch/val): 0.1282/0.1259. Time elapsed: 77.91\n",
      "Epoch [36/600], Loss (last training batch/val): 0.1251/0.1226. Time elapsed: 80.14\n",
      "Epoch [37/600], Loss (last training batch/val): 0.1219/0.1217. Time elapsed: 82.32\n",
      "Epoch [38/600], Loss (last training batch/val): 0.1196/0.1182. Time elapsed: 84.54\n",
      "Epoch [39/600], Loss (last training batch/val): 0.1161/0.1168. Time elapsed: 86.78\n",
      "Epoch [40/600], Loss (last training batch/val): 0.1155/0.1151. Time elapsed: 88.98\n",
      "Epoch [41/600], Loss (last training batch/val): 0.1144/0.1134. Time elapsed: 91.16\n",
      "Epoch [42/600], Loss (last training batch/val): 0.1115/0.1108. Time elapsed: 93.35\n",
      "Epoch [43/600], Loss (last training batch/val): 0.1109/0.1099. Time elapsed: 95.60\n",
      "Epoch [44/600], Loss (last training batch/val): 0.1091/0.1083. Time elapsed: 97.80\n",
      "Epoch [45/600], Loss (last training batch/val): 0.1084/0.1071. Time elapsed: 99.99\n",
      "Epoch [46/600], Loss (last training batch/val): 0.1082/0.1067. Time elapsed: 102.19\n",
      "Epoch [47/600], Loss (last training batch/val): 0.1057/0.1048. Time elapsed: 104.34\n",
      "Epoch [48/600], Loss (last training batch/val): 0.1060/0.1041. Time elapsed: 106.56\n",
      "Epoch [49/600], Loss (last training batch/val): 0.1035/0.1027. Time elapsed: 108.76\n",
      "Epoch [50/600], Loss (last training batch/val): 0.1033/0.1019. Time elapsed: 111.16\n",
      "Epoch [51/600], Loss (last training batch/val): 0.1022/0.1011. Time elapsed: 113.35\n",
      "Epoch [52/600], Loss (last training batch/val): 0.1016/0.0997. Time elapsed: 115.57\n",
      "Epoch [53/600], Loss (last training batch/val): 0.0999/0.0987. Time elapsed: 117.80\n",
      "Epoch [54/600], Loss (last training batch/val): 0.0984/0.0979. Time elapsed: 120.04\n",
      "Epoch [55/600], Loss (last training batch/val): 0.0988/0.0974. Time elapsed: 122.26\n",
      "Epoch [56/600], Loss (last training batch/val): 0.0968/0.0967. Time elapsed: 124.50\n",
      "Epoch [57/600], Loss (last training batch/val): 0.0980/0.0953. Time elapsed: 126.68\n",
      "Epoch [58/600], Loss (last training batch/val): 0.0950/0.0939. Time elapsed: 128.86\n",
      "Epoch [59/600], Loss (last training batch/val): 0.0944/0.0929. Time elapsed: 131.09\n",
      "Epoch [60/600], Loss (last training batch/val): 0.0953/0.0941. Time elapsed: 133.35\n",
      "Epoch [61/600], Loss (last training batch/val): 0.0922/0.0911. Time elapsed: 135.59\n",
      "Epoch [62/600], Loss (last training batch/val): 0.0940/0.0913. Time elapsed: 137.86\n",
      "Epoch [63/600], Loss (last training batch/val): 0.0900/0.0896. Time elapsed: 140.10\n",
      "Epoch [64/600], Loss (last training batch/val): 0.0908/0.0905. Time elapsed: 142.40\n",
      "Epoch [65/600], Loss (last training batch/val): 0.0901/0.0891. Time elapsed: 144.63\n",
      "Epoch [66/600], Loss (last training batch/val): 0.0889/0.0867. Time elapsed: 146.93\n",
      "Epoch [67/600], Loss (last training batch/val): 0.0871/0.0857. Time elapsed: 149.35\n",
      "Epoch [68/600], Loss (last training batch/val): 0.0874/0.0852. Time elapsed: 152.03\n",
      "Epoch [69/600], Loss (last training batch/val): 0.0851/0.0836. Time elapsed: 154.52\n",
      "Epoch [70/600], Loss (last training batch/val): 0.0867/0.0830. Time elapsed: 156.94\n",
      "Epoch [71/600], Loss (last training batch/val): 0.0846/0.0828. Time elapsed: 159.25\n",
      "Epoch [72/600], Loss (last training batch/val): 0.0860/0.0819. Time elapsed: 161.50\n",
      "Epoch [73/600], Loss (last training batch/val): 0.0837/0.0814. Time elapsed: 163.74\n",
      "Epoch [74/600], Loss (last training batch/val): 0.0817/0.0807. Time elapsed: 166.00\n",
      "Epoch [75/600], Loss (last training batch/val): 0.0823/0.0800. Time elapsed: 168.20\n",
      "Epoch [76/600], Loss (last training batch/val): 0.0831/0.0805. Time elapsed: 170.35\n",
      "Epoch [77/600], Loss (last training batch/val): 0.0795/0.0777. Time elapsed: 172.49\n",
      "Epoch [78/600], Loss (last training batch/val): 0.0784/0.0774. Time elapsed: 174.67\n",
      "Epoch [79/600], Loss (last training batch/val): 0.0798/0.0778. Time elapsed: 176.78\n",
      "Epoch [80/600], Loss (last training batch/val): 0.0798/0.0771. Time elapsed: 178.91\n",
      "Epoch [81/600], Loss (last training batch/val): 0.0803/0.0751. Time elapsed: 181.04\n",
      "Epoch [82/600], Loss (last training batch/val): 0.0782/0.0761. Time elapsed: 183.20\n",
      "Epoch [83/600], Loss (last training batch/val): 0.0783/0.0764. Time elapsed: 185.30\n",
      "Epoch [84/600], Loss (last training batch/val): 0.0812/0.0815. Time elapsed: 187.45\n",
      "Epoch [85/600], Loss (last training batch/val): 0.0768/0.0747. Time elapsed: 189.64\n",
      "Epoch [86/600], Loss (last training batch/val): 0.0741/0.0719. Time elapsed: 191.98\n",
      "Epoch [87/600], Loss (last training batch/val): 0.0770/0.0737. Time elapsed: 194.16\n",
      "Epoch [88/600], Loss (last training batch/val): 0.0739/0.0720. Time elapsed: 196.30\n",
      "Epoch [89/600], Loss (last training batch/val): 0.0753/0.0725. Time elapsed: 198.44\n",
      "Epoch [90/600], Loss (last training batch/val): 0.0768/0.0751. Time elapsed: 200.58\n",
      "Epoch [91/600], Loss (last training batch/val): 0.0730/0.0707. Time elapsed: 202.69\n",
      "Epoch [92/600], Loss (last training batch/val): 0.0729/0.0690. Time elapsed: 204.81\n",
      "Epoch [93/600], Loss (last training batch/val): 0.0741/0.0736. Time elapsed: 206.95\n",
      "Epoch [94/600], Loss (last training batch/val): 0.0725/0.0696. Time elapsed: 209.17\n",
      "Epoch [95/600], Loss (last training batch/val): 0.0792/0.0793. Time elapsed: 211.46\n",
      "Epoch [96/600], Loss (last training batch/val): 0.0732/0.0722. Time elapsed: 213.71\n",
      "Epoch [97/600], Loss (last training batch/val): 0.0719/0.0705. Time elapsed: 215.84\n",
      "Epoch [98/600], Loss (last training batch/val): 0.0677/0.0658. Time elapsed: 217.95\n",
      "Epoch [99/600], Loss (last training batch/val): 0.0741/0.0727. Time elapsed: 220.06\n",
      "Epoch [100/600], Loss (last training batch/val): 0.0776/0.0776. Time elapsed: 222.26\n",
      "Epoch [101/600], Loss (last training batch/val): 0.0694/0.0674. Time elapsed: 224.48\n",
      "Epoch [102/600], Loss (last training batch/val): 0.0683/0.0659. Time elapsed: 226.72\n",
      "Epoch [103/600], Loss (last training batch/val): 0.0722/0.0696. Time elapsed: 228.99\n",
      "Epoch   104: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch [104/600], Loss (last training batch/val): 0.0706/0.0706. Time elapsed: 231.18\n",
      "Epoch [105/600], Loss (last training batch/val): 0.0663/0.0609. Time elapsed: 233.27\n",
      "Epoch [106/600], Loss (last training batch/val): 0.0648/0.0601. Time elapsed: 235.37\n",
      "Epoch [107/600], Loss (last training batch/val): 0.0636/0.0589. Time elapsed: 237.50\n",
      "Epoch [108/600], Loss (last training batch/val): 0.0620/0.0582. Time elapsed: 239.71\n",
      "Epoch [109/600], Loss (last training batch/val): 0.0627/0.0571. Time elapsed: 241.97\n",
      "Epoch [110/600], Loss (last training batch/val): 0.0615/0.0562. Time elapsed: 244.08\n",
      "Epoch [111/600], Loss (last training batch/val): 0.0600/0.0555. Time elapsed: 246.33\n",
      "Epoch [112/600], Loss (last training batch/val): 0.0598/0.0552. Time elapsed: 248.54\n",
      "Epoch [113/600], Loss (last training batch/val): 0.0588/0.0539. Time elapsed: 250.66\n",
      "Epoch [114/600], Loss (last training batch/val): 0.0608/0.0564. Time elapsed: 252.77\n",
      "Epoch [115/600], Loss (last training batch/val): 0.0581/0.0529. Time elapsed: 254.95\n",
      "Epoch [116/600], Loss (last training batch/val): 0.0579/0.0524. Time elapsed: 257.09\n",
      "Epoch [117/600], Loss (last training batch/val): 0.0571/0.0522. Time elapsed: 259.25\n",
      "Epoch [118/600], Loss (last training batch/val): 0.0563/0.0517. Time elapsed: 261.45\n",
      "Epoch [119/600], Loss (last training batch/val): 0.0558/0.0512. Time elapsed: 263.64\n",
      "Epoch [120/600], Loss (last training batch/val): 0.0571/0.0516. Time elapsed: 265.86\n",
      "Epoch [121/600], Loss (last training batch/val): 0.0551/0.0509. Time elapsed: 267.96\n",
      "Epoch [122/600], Loss (last training batch/val): 0.0562/0.0501. Time elapsed: 270.68\n",
      "Epoch [123/600], Loss (last training batch/val): 0.0552/0.0512. Time elapsed: 272.90\n",
      "Epoch [124/600], Loss (last training batch/val): 0.0552/0.0498. Time elapsed: 275.01\n",
      "Epoch [125/600], Loss (last training batch/val): 0.0554/0.0493. Time elapsed: 277.12\n",
      "Epoch [126/600], Loss (last training batch/val): 0.0556/0.0491. Time elapsed: 279.21\n",
      "Epoch [127/600], Loss (last training batch/val): 0.0542/0.0490. Time elapsed: 281.30\n",
      "Epoch [128/600], Loss (last training batch/val): 0.0543/0.0480. Time elapsed: 283.45\n",
      "Epoch [129/600], Loss (last training batch/val): 0.0530/0.0477. Time elapsed: 285.64\n",
      "Epoch [130/600], Loss (last training batch/val): 0.0537/0.0474. Time elapsed: 287.78\n",
      "Epoch [131/600], Loss (last training batch/val): 0.0551/0.0481. Time elapsed: 289.92\n",
      "Epoch [132/600], Loss (last training batch/val): 0.0521/0.0476. Time elapsed: 292.02\n",
      "Epoch [133/600], Loss (last training batch/val): 0.0527/0.0491. Time elapsed: 294.20\n",
      "Epoch [134/600], Loss (last training batch/val): 0.0535/0.0474. Time elapsed: 296.34\n",
      "Epoch [135/600], Loss (last training batch/val): 0.0534/0.0470. Time elapsed: 298.49\n",
      "Epoch [136/600], Loss (last training batch/val): 0.0521/0.0469. Time elapsed: 300.64\n",
      "Epoch [137/600], Loss (last training batch/val): 0.0513/0.0457. Time elapsed: 302.84\n",
      "Epoch [138/600], Loss (last training batch/val): 0.0523/0.0470. Time elapsed: 304.96\n",
      "Epoch [139/600], Loss (last training batch/val): 0.0518/0.0451. Time elapsed: 307.07\n",
      "Epoch [140/600], Loss (last training batch/val): 0.0503/0.0444. Time elapsed: 309.22\n",
      "Epoch [141/600], Loss (last training batch/val): 0.0559/0.0515. Time elapsed: 311.37\n",
      "Epoch [142/600], Loss (last training batch/val): 0.0529/0.0460. Time elapsed: 313.57\n",
      "Epoch [143/600], Loss (last training batch/val): 0.0511/0.0444. Time elapsed: 315.67\n",
      "Epoch [144/600], Loss (last training batch/val): 0.0511/0.0455. Time elapsed: 317.91\n",
      "Epoch [145/600], Loss (last training batch/val): 0.0504/0.0436. Time elapsed: 320.05\n",
      "Epoch [146/600], Loss (last training batch/val): 0.0505/0.0441. Time elapsed: 322.24\n",
      "Epoch [147/600], Loss (last training batch/val): 0.0507/0.0451. Time elapsed: 324.59\n",
      "Epoch [148/600], Loss (last training batch/val): 0.0483/0.0427. Time elapsed: 327.06\n",
      "Epoch [149/600], Loss (last training batch/val): 0.0498/0.0426. Time elapsed: 329.42\n",
      "Epoch [150/600], Loss (last training batch/val): 0.0499/0.0452. Time elapsed: 331.76\n",
      "Epoch [151/600], Loss (last training batch/val): 0.0492/0.0427. Time elapsed: 334.17\n",
      "Epoch [152/600], Loss (last training batch/val): 0.0473/0.0429. Time elapsed: 336.49\n",
      "Epoch [153/600], Loss (last training batch/val): 0.0487/0.0418. Time elapsed: 338.72\n",
      "Epoch [154/600], Loss (last training batch/val): 0.0485/0.0429. Time elapsed: 341.11\n",
      "Epoch [155/600], Loss (last training batch/val): 0.0490/0.0421. Time elapsed: 343.43\n",
      "Epoch [156/600], Loss (last training batch/val): 0.0470/0.0415. Time elapsed: 345.66\n",
      "Epoch [157/600], Loss (last training batch/val): 0.0519/0.0451. Time elapsed: 347.89\n",
      "Epoch [158/600], Loss (last training batch/val): 0.0514/0.0447. Time elapsed: 350.03\n",
      "Epoch [159/600], Loss (last training batch/val): 0.0476/0.0410. Time elapsed: 352.16\n",
      "Epoch [160/600], Loss (last training batch/val): 0.0478/0.0406. Time elapsed: 354.27\n",
      "Epoch [161/600], Loss (last training batch/val): 0.0479/0.0423. Time elapsed: 356.38\n",
      "Epoch [162/600], Loss (last training batch/val): 0.0478/0.0402. Time elapsed: 358.49\n",
      "Epoch [163/600], Loss (last training batch/val): 0.0489/0.0424. Time elapsed: 360.60\n",
      "Epoch [164/600], Loss (last training batch/val): 0.0469/0.0401. Time elapsed: 362.87\n",
      "Epoch [165/600], Loss (last training batch/val): 0.0485/0.0400. Time elapsed: 365.02\n",
      "Epoch [166/600], Loss (last training batch/val): 0.0491/0.0407. Time elapsed: 367.34\n",
      "Epoch [167/600], Loss (last training batch/val): 0.0465/0.0400. Time elapsed: 369.58\n",
      "Epoch [168/600], Loss (last training batch/val): 0.0476/0.0410. Time elapsed: 371.70\n",
      "Epoch [169/600], Loss (last training batch/val): 0.0470/0.0395. Time elapsed: 373.85\n",
      "Epoch [170/600], Loss (last training batch/val): 0.0459/0.0398. Time elapsed: 375.98\n",
      "Epoch [171/600], Loss (last training batch/val): 0.0485/0.0394. Time elapsed: 378.09\n",
      "Epoch [172/600], Loss (last training batch/val): 0.0472/0.0394. Time elapsed: 380.18\n",
      "Epoch [173/600], Loss (last training batch/val): 0.0466/0.0405. Time elapsed: 382.28\n",
      "Epoch [174/600], Loss (last training batch/val): 0.0455/0.0385. Time elapsed: 384.37\n",
      "Epoch [175/600], Loss (last training batch/val): 0.0446/0.0385. Time elapsed: 386.48\n",
      "Epoch [176/600], Loss (last training batch/val): 0.0442/0.0384. Time elapsed: 388.57\n",
      "Epoch [177/600], Loss (last training batch/val): 0.0460/0.0382. Time elapsed: 390.69\n",
      "Epoch [178/600], Loss (last training batch/val): 0.0449/0.0392. Time elapsed: 392.87\n",
      "Epoch [179/600], Loss (last training batch/val): 0.0453/0.0391. Time elapsed: 395.04\n",
      "Epoch [180/600], Loss (last training batch/val): 0.0452/0.0380. Time elapsed: 397.24\n",
      "Epoch [181/600], Loss (last training batch/val): 0.0460/0.0377. Time elapsed: 399.37\n",
      "Epoch [182/600], Loss (last training batch/val): 0.0452/0.0379. Time elapsed: 401.52\n",
      "Epoch [183/600], Loss (last training batch/val): 0.0442/0.0376. Time elapsed: 403.65\n",
      "Epoch [184/600], Loss (last training batch/val): 0.0424/0.0375. Time elapsed: 405.79\n",
      "Epoch [185/600], Loss (last training batch/val): 0.0448/0.0374. Time elapsed: 408.29\n",
      "Epoch [186/600], Loss (last training batch/val): 0.0442/0.0374. Time elapsed: 410.40\n",
      "Epoch [187/600], Loss (last training batch/val): 0.0443/0.0372. Time elapsed: 412.56\n",
      "Epoch [188/600], Loss (last training batch/val): 0.0466/0.0379. Time elapsed: 414.69\n",
      "Epoch [189/600], Loss (last training batch/val): 0.0448/0.0371. Time elapsed: 416.83\n",
      "Epoch [190/600], Loss (last training batch/val): 0.0426/0.0374. Time elapsed: 418.92\n",
      "Epoch [191/600], Loss (last training batch/val): 0.0447/0.0369. Time elapsed: 421.05\n",
      "Epoch [192/600], Loss (last training batch/val): 0.0442/0.0369. Time elapsed: 423.20\n",
      "Epoch [193/600], Loss (last training batch/val): 0.0443/0.0367. Time elapsed: 425.32\n",
      "Epoch [194/600], Loss (last training batch/val): 0.0453/0.0372. Time elapsed: 427.58\n",
      "Epoch [195/600], Loss (last training batch/val): 0.0426/0.0380. Time elapsed: 429.67\n",
      "Epoch [196/600], Loss (last training batch/val): 0.0439/0.0363. Time elapsed: 431.80\n",
      "Epoch [197/600], Loss (last training batch/val): 0.0431/0.0363. Time elapsed: 434.00\n",
      "Epoch [198/600], Loss (last training batch/val): 0.0457/0.0370. Time elapsed: 436.18\n",
      "Epoch [199/600], Loss (last training batch/val): 0.0422/0.0367. Time elapsed: 438.31\n",
      "Epoch [200/600], Loss (last training batch/val): 0.0443/0.0380. Time elapsed: 440.59\n",
      "Epoch [201/600], Loss (last training batch/val): 0.0450/0.0372. Time elapsed: 442.91\n",
      "Epoch [202/600], Loss (last training batch/val): 0.0451/0.0365. Time elapsed: 445.06\n",
      "Epoch   203: reducing learning rate of group 0 to 2.5000e-03.\n",
      "Epoch [203/600], Loss (last training batch/val): 0.0440/0.0366. Time elapsed: 447.23\n",
      "Epoch [204/600], Loss (last training batch/val): 0.0434/0.0358. Time elapsed: 449.39\n",
      "Epoch [205/600], Loss (last training batch/val): 0.0434/0.0357. Time elapsed: 451.50\n",
      "Epoch [206/600], Loss (last training batch/val): 0.0443/0.0357. Time elapsed: 453.60\n",
      "Epoch [207/600], Loss (last training batch/val): 0.0437/0.0357. Time elapsed: 455.76\n",
      "Epoch [208/600], Loss (last training batch/val): 0.0437/0.0356. Time elapsed: 457.88\n",
      "Epoch [209/600], Loss (last training batch/val): 0.0449/0.0357. Time elapsed: 459.98\n",
      "Epoch [210/600], Loss (last training batch/val): 0.0427/0.0357. Time elapsed: 462.08\n",
      "Epoch [211/600], Loss (last training batch/val): 0.0438/0.0355. Time elapsed: 464.19\n",
      "Epoch [212/600], Loss (last training batch/val): 0.0443/0.0355. Time elapsed: 466.29\n",
      "Epoch [213/600], Loss (last training batch/val): 0.0427/0.0355. Time elapsed: 468.46\n",
      "Epoch [214/600], Loss (last training batch/val): 0.0447/0.0355. Time elapsed: 470.62\n",
      "Epoch [215/600], Loss (last training batch/val): 0.0443/0.0353. Time elapsed: 472.73\n",
      "Epoch [216/600], Loss (last training batch/val): 0.0426/0.0353. Time elapsed: 474.80\n",
      "Epoch [217/600], Loss (last training batch/val): 0.0424/0.0352. Time elapsed: 476.88\n",
      "Epoch [218/600], Loss (last training batch/val): 0.0428/0.0355. Time elapsed: 478.97\n",
      "Epoch [219/600], Loss (last training batch/val): 0.0452/0.0355. Time elapsed: 481.07\n",
      "Epoch [220/600], Loss (last training batch/val): 0.0449/0.0352. Time elapsed: 483.16\n",
      "Epoch [221/600], Loss (last training batch/val): 0.0417/0.0352. Time elapsed: 485.23\n",
      "Epoch [222/600], Loss (last training batch/val): 0.0434/0.0349. Time elapsed: 487.31\n",
      "Epoch [223/600], Loss (last training batch/val): 0.0449/0.0350. Time elapsed: 489.43\n",
      "Epoch [224/600], Loss (last training batch/val): 0.0435/0.0350. Time elapsed: 491.54\n",
      "Epoch [225/600], Loss (last training batch/val): 0.0431/0.0349. Time elapsed: 493.65\n",
      "Epoch [226/600], Loss (last training batch/val): 0.0441/0.0353. Time elapsed: 495.74\n",
      "Epoch [227/600], Loss (last training batch/val): 0.0424/0.0351. Time elapsed: 497.86\n",
      "Epoch [228/600], Loss (last training batch/val): 0.0433/0.0347. Time elapsed: 499.96\n",
      "Epoch [229/600], Loss (last training batch/val): 0.0425/0.0352. Time elapsed: 502.04\n",
      "Epoch [230/600], Loss (last training batch/val): 0.0429/0.0350. Time elapsed: 504.14\n",
      "Epoch [231/600], Loss (last training batch/val): 0.0421/0.0347. Time elapsed: 506.23\n",
      "Epoch [232/600], Loss (last training batch/val): 0.0443/0.0347. Time elapsed: 508.31\n",
      "Epoch [233/600], Loss (last training batch/val): 0.0414/0.0346. Time elapsed: 510.41\n",
      "Epoch [234/600], Loss (last training batch/val): 0.0432/0.0348. Time elapsed: 512.54\n",
      "Epoch [235/600], Loss (last training batch/val): 0.0441/0.0348. Time elapsed: 514.64\n",
      "Epoch [236/600], Loss (last training batch/val): 0.0421/0.0346. Time elapsed: 516.76\n",
      "Epoch [237/600], Loss (last training batch/val): 0.0424/0.0345. Time elapsed: 518.84\n",
      "Epoch [238/600], Loss (last training batch/val): 0.0418/0.0345. Time elapsed: 520.96\n",
      "Epoch [239/600], Loss (last training batch/val): 0.0422/0.0346. Time elapsed: 523.05\n",
      "Epoch [240/600], Loss (last training batch/val): 0.0414/0.0343. Time elapsed: 525.19\n",
      "Epoch [241/600], Loss (last training batch/val): 0.0415/0.0343. Time elapsed: 527.31\n",
      "Epoch [242/600], Loss (last training batch/val): 0.0413/0.0344. Time elapsed: 529.45\n",
      "Epoch [243/600], Loss (last training batch/val): 0.0428/0.0345. Time elapsed: 531.54\n",
      "Epoch [244/600], Loss (last training batch/val): 0.0427/0.0348. Time elapsed: 533.63\n",
      "Epoch [245/600], Loss (last training batch/val): 0.0421/0.0342. Time elapsed: 535.73\n",
      "Epoch [246/600], Loss (last training batch/val): 0.0426/0.0343. Time elapsed: 537.83\n",
      "Epoch [247/600], Loss (last training batch/val): 0.0430/0.0342. Time elapsed: 539.93\n",
      "Epoch [248/600], Loss (last training batch/val): 0.0408/0.0343. Time elapsed: 542.01\n",
      "Epoch [249/600], Loss (last training batch/val): 0.0424/0.0342. Time elapsed: 544.10\n",
      "Epoch [250/600], Loss (last training batch/val): 0.0409/0.0345. Time elapsed: 546.26\n",
      "Epoch [251/600], Loss (last training batch/val): 0.0426/0.0341. Time elapsed: 548.40\n",
      "Epoch [252/600], Loss (last training batch/val): 0.0421/0.0343. Time elapsed: 550.50\n",
      "Epoch [253/600], Loss (last training batch/val): 0.0421/0.0347. Time elapsed: 552.63\n",
      "Epoch [254/600], Loss (last training batch/val): 0.0419/0.0345. Time elapsed: 554.75\n",
      "Epoch [255/600], Loss (last training batch/val): 0.0421/0.0344. Time elapsed: 556.86\n",
      "Epoch [256/600], Loss (last training batch/val): 0.0417/0.0341. Time elapsed: 558.98\n",
      "Epoch [257/600], Loss (last training batch/val): 0.0431/0.0342. Time elapsed: 561.10\n",
      "Epoch [258/600], Loss (last training batch/val): 0.0399/0.0337. Time elapsed: 563.20\n",
      "Epoch [259/600], Loss (last training batch/val): 0.0415/0.0347. Time elapsed: 565.28\n",
      "Epoch [260/600], Loss (last training batch/val): 0.0416/0.0341. Time elapsed: 567.37\n",
      "Epoch [261/600], Loss (last training batch/val): 0.0412/0.0339. Time elapsed: 569.51\n",
      "Epoch [262/600], Loss (last training batch/val): 0.0409/0.0336. Time elapsed: 571.65\n",
      "Epoch [263/600], Loss (last training batch/val): 0.0424/0.0343. Time elapsed: 573.73\n",
      "Epoch [264/600], Loss (last training batch/val): 0.0406/0.0334. Time elapsed: 575.82\n",
      "Epoch [265/600], Loss (last training batch/val): 0.0400/0.0334. Time elapsed: 577.97\n",
      "Epoch [266/600], Loss (last training batch/val): 0.0408/0.0333. Time elapsed: 580.09\n",
      "Epoch [267/600], Loss (last training batch/val): 0.0422/0.0338. Time elapsed: 582.24\n",
      "Epoch [268/600], Loss (last training batch/val): 0.0414/0.0337. Time elapsed: 584.39\n",
      "Epoch [269/600], Loss (last training batch/val): 0.0410/0.0336. Time elapsed: 586.49\n",
      "Epoch [270/600], Loss (last training batch/val): 0.0410/0.0333. Time elapsed: 588.62\n",
      "Epoch [271/600], Loss (last training batch/val): 0.0408/0.0337. Time elapsed: 590.72\n",
      "Epoch   272: reducing learning rate of group 0 to 1.2500e-03.\n",
      "Epoch [272/600], Loss (last training batch/val): 0.0432/0.0338. Time elapsed: 592.82\n",
      "Epoch [273/600], Loss (last training batch/val): 0.0397/0.0332. Time elapsed: 594.96\n",
      "Epoch [274/600], Loss (last training batch/val): 0.0396/0.0332. Time elapsed: 597.08\n",
      "Epoch [275/600], Loss (last training batch/val): 0.0421/0.0334. Time elapsed: 599.22\n",
      "Epoch [276/600], Loss (last training batch/val): 0.0426/0.0332. Time elapsed: 601.39\n",
      "Epoch [277/600], Loss (last training batch/val): 0.0395/0.0332. Time elapsed: 603.52\n",
      "Epoch [278/600], Loss (last training batch/val): 0.0405/0.0332. Time elapsed: 605.66\n",
      "Epoch [279/600], Loss (last training batch/val): 0.0423/0.0331. Time elapsed: 607.77\n",
      "Epoch [280/600], Loss (last training batch/val): 0.0392/0.0331. Time elapsed: 609.91\n",
      "Epoch [281/600], Loss (last training batch/val): 0.0409/0.0331. Time elapsed: 612.05\n",
      "Epoch [282/600], Loss (last training batch/val): 0.0411/0.0332. Time elapsed: 614.15\n",
      "Epoch [283/600], Loss (last training batch/val): 0.0411/0.0330. Time elapsed: 616.30\n",
      "Epoch [284/600], Loss (last training batch/val): 0.0402/0.0332. Time elapsed: 618.40\n",
      "Epoch [285/600], Loss (last training batch/val): 0.0418/0.0329. Time elapsed: 620.51\n",
      "Epoch [286/600], Loss (last training batch/val): 0.0421/0.0330. Time elapsed: 622.61\n",
      "Epoch [287/600], Loss (last training batch/val): 0.0399/0.0328. Time elapsed: 624.72\n",
      "Epoch [288/600], Loss (last training batch/val): 0.0423/0.0329. Time elapsed: 626.86\n",
      "Epoch [289/600], Loss (last training batch/val): 0.0422/0.0329. Time elapsed: 629.02\n",
      "Epoch [290/600], Loss (last training batch/val): 0.0401/0.0328. Time elapsed: 631.14\n",
      "Epoch [291/600], Loss (last training batch/val): 0.0412/0.0330. Time elapsed: 633.27\n",
      "Epoch [292/600], Loss (last training batch/val): 0.0415/0.0329. Time elapsed: 635.37\n",
      "Epoch   293: reducing learning rate of group 0 to 6.2500e-04.\n",
      "Epoch [293/600], Loss (last training batch/val): 0.0398/0.0330. Time elapsed: 637.50\n",
      "Epoch [294/600], Loss (last training batch/val): 0.0414/0.0328. Time elapsed: 639.59\n",
      "Epoch [295/600], Loss (last training batch/val): 0.0412/0.0329. Time elapsed: 641.77\n",
      "Epoch [296/600], Loss (last training batch/val): 0.0418/0.0328. Time elapsed: 643.91\n",
      "Epoch [297/600], Loss (last training batch/val): 0.0424/0.0329. Time elapsed: 646.03\n",
      "Epoch [298/600], Loss (last training batch/val): 0.0418/0.0328. Time elapsed: 648.16\n",
      "Epoch [299/600], Loss (last training batch/val): 0.0425/0.0328. Time elapsed: 650.29\n",
      "Epoch [300/600], Loss (last training batch/val): 0.0418/0.0328. Time elapsed: 652.39\n",
      "Epoch [301/600], Loss (last training batch/val): 0.0427/0.0328. Time elapsed: 654.55\n",
      "Epoch [302/600], Loss (last training batch/val): 0.0377/0.0327. Time elapsed: 656.68\n",
      "Epoch [303/600], Loss (last training batch/val): 0.0399/0.0327. Time elapsed: 658.79\n",
      "Epoch [304/600], Loss (last training batch/val): 0.0405/0.0327. Time elapsed: 660.90\n",
      "Epoch [305/600], Loss (last training batch/val): 0.0411/0.0328. Time elapsed: 663.00\n",
      "Epoch [306/600], Loss (last training batch/val): 0.0407/0.0327. Time elapsed: 665.11\n",
      "Epoch [307/600], Loss (last training batch/val): 0.0407/0.0327. Time elapsed: 667.24\n",
      "Epoch [308/600], Loss (last training batch/val): 0.0411/0.0328. Time elapsed: 669.37\n",
      "Epoch [309/600], Loss (last training batch/val): 0.0408/0.0328. Time elapsed: 671.46\n",
      "Epoch [310/600], Loss (last training batch/val): 0.0411/0.0327. Time elapsed: 673.57\n",
      "Epoch [311/600], Loss (last training batch/val): 0.0413/0.0326. Time elapsed: 675.68\n",
      "Epoch [312/600], Loss (last training batch/val): 0.0411/0.0327. Time elapsed: 677.78\n",
      "Epoch [313/600], Loss (last training batch/val): 0.0395/0.0327. Time elapsed: 679.91\n",
      "Epoch [314/600], Loss (last training batch/val): 0.0413/0.0327. Time elapsed: 682.02\n",
      "Epoch [315/600], Loss (last training batch/val): 0.0415/0.0326. Time elapsed: 684.11\n",
      "Epoch [316/600], Loss (last training batch/val): 0.0420/0.0326. Time elapsed: 686.29\n",
      "Epoch [317/600], Loss (last training batch/val): 0.0403/0.0327. Time elapsed: 688.42\n",
      "Epoch [318/600], Loss (last training batch/val): 0.0405/0.0326. Time elapsed: 690.52\n",
      "Epoch [319/600], Loss (last training batch/val): 0.0402/0.0326. Time elapsed: 692.70\n",
      "Epoch [320/600], Loss (last training batch/val): 0.0411/0.0327. Time elapsed: 694.82\n",
      "Epoch [321/600], Loss (last training batch/val): 0.0396/0.0327. Time elapsed: 696.92\n",
      "Epoch [322/600], Loss (last training batch/val): 0.0403/0.0328. Time elapsed: 699.02\n",
      "Epoch [323/600], Loss (last training batch/val): 0.0402/0.0327. Time elapsed: 701.17\n",
      "Epoch   324: reducing learning rate of group 0 to 3.1250e-04.\n",
      "Epoch [324/600], Loss (last training batch/val): 0.0397/0.0326. Time elapsed: 703.39\n",
      "Epoch [325/600], Loss (last training batch/val): 0.0401/0.0325. Time elapsed: 705.57\n",
      "Epoch [326/600], Loss (last training batch/val): 0.0422/0.0325. Time elapsed: 707.83\n",
      "Epoch [327/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 710.43\n",
      "Epoch [328/600], Loss (last training batch/val): 0.0403/0.0326. Time elapsed: 712.62\n",
      "Epoch [329/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 714.91\n",
      "Epoch [330/600], Loss (last training batch/val): 0.0399/0.0326. Time elapsed: 717.08\n",
      "Epoch [331/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 719.21\n",
      "Epoch   332: reducing learning rate of group 0 to 1.5625e-04.\n",
      "Epoch [332/600], Loss (last training batch/val): 0.0401/0.0326. Time elapsed: 721.35\n",
      "Epoch [333/600], Loss (last training batch/val): 0.0414/0.0326. Time elapsed: 723.48\n",
      "Epoch [334/600], Loss (last training batch/val): 0.0405/0.0325. Time elapsed: 725.63\n",
      "Epoch [335/600], Loss (last training batch/val): 0.0395/0.0326. Time elapsed: 727.81\n",
      "Epoch [336/600], Loss (last training batch/val): 0.0408/0.0326. Time elapsed: 729.98\n",
      "Epoch [337/600], Loss (last training batch/val): 0.0411/0.0325. Time elapsed: 732.27\n",
      "Epoch [338/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 734.44\n",
      "Epoch [339/600], Loss (last training batch/val): 0.0400/0.0325. Time elapsed: 736.56\n",
      "Epoch [340/600], Loss (last training batch/val): 0.0400/0.0325. Time elapsed: 738.70\n",
      "Epoch [341/600], Loss (last training batch/val): 0.0419/0.0326. Time elapsed: 740.88\n",
      "Epoch [342/600], Loss (last training batch/val): 0.0403/0.0325. Time elapsed: 743.09\n",
      "Epoch [343/600], Loss (last training batch/val): 0.0393/0.0325. Time elapsed: 745.45\n",
      "Epoch   344: reducing learning rate of group 0 to 7.8125e-05.\n",
      "Epoch [344/600], Loss (last training batch/val): 0.0411/0.0325. Time elapsed: 747.82\n",
      "Epoch [345/600], Loss (last training batch/val): 0.0405/0.0325. Time elapsed: 750.02\n",
      "Epoch [346/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 752.24\n",
      "Epoch [347/600], Loss (last training batch/val): 0.0407/0.0325. Time elapsed: 754.41\n",
      "Epoch [348/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 756.60\n",
      "Epoch [349/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 758.75\n",
      "Epoch [350/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 760.86\n",
      "Epoch [351/600], Loss (last training batch/val): 0.0398/0.0325. Time elapsed: 762.99\n",
      "Epoch   352: reducing learning rate of group 0 to 3.9063e-05.\n",
      "Epoch [352/600], Loss (last training batch/val): 0.0395/0.0325. Time elapsed: 765.19\n",
      "Epoch [353/600], Loss (last training batch/val): 0.0428/0.0325. Time elapsed: 767.31\n",
      "Epoch [354/600], Loss (last training batch/val): 0.0394/0.0325. Time elapsed: 769.42\n",
      "Epoch [355/600], Loss (last training batch/val): 0.0394/0.0325. Time elapsed: 771.52\n",
      "Epoch [356/600], Loss (last training batch/val): 0.0412/0.0325. Time elapsed: 773.67\n",
      "Epoch [357/600], Loss (last training batch/val): 0.0401/0.0325. Time elapsed: 775.84\n",
      "Epoch [358/600], Loss (last training batch/val): 0.0412/0.0325. Time elapsed: 778.00\n",
      "Epoch [359/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 780.12\n",
      "Epoch [360/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 782.35\n",
      "Epoch   361: reducing learning rate of group 0 to 1.9531e-05.\n",
      "Epoch [361/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 784.50\n",
      "Epoch [362/600], Loss (last training batch/val): 0.0410/0.0325. Time elapsed: 786.64\n",
      "Epoch [363/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 788.79\n",
      "Epoch [364/600], Loss (last training batch/val): 0.0412/0.0325. Time elapsed: 790.92\n",
      "Epoch [365/600], Loss (last training batch/val): 0.0402/0.0325. Time elapsed: 793.14\n",
      "Epoch [366/600], Loss (last training batch/val): 0.0428/0.0325. Time elapsed: 795.49\n",
      "Epoch [367/600], Loss (last training batch/val): 0.0421/0.0325. Time elapsed: 797.76\n",
      "Epoch [368/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 800.07\n",
      "Epoch   369: reducing learning rate of group 0 to 9.7656e-06.\n",
      "Epoch [369/600], Loss (last training batch/val): 0.0400/0.0325. Time elapsed: 802.22\n",
      "Epoch [370/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 804.31\n",
      "Epoch [371/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 806.46\n",
      "Epoch [372/600], Loss (last training batch/val): 0.0392/0.0325. Time elapsed: 808.62\n",
      "Epoch [373/600], Loss (last training batch/val): 0.0398/0.0325. Time elapsed: 810.74\n",
      "Epoch [374/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 812.86\n",
      "Epoch [375/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 814.97\n",
      "Epoch [376/600], Loss (last training batch/val): 0.0392/0.0325. Time elapsed: 817.15\n",
      "Epoch   377: reducing learning rate of group 0 to 4.8828e-06.\n",
      "Epoch [377/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 819.25\n",
      "Epoch [378/600], Loss (last training batch/val): 0.0411/0.0325. Time elapsed: 821.48\n",
      "Epoch [379/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 823.62\n",
      "Epoch [380/600], Loss (last training batch/val): 0.0421/0.0325. Time elapsed: 825.73\n",
      "Epoch [381/600], Loss (last training batch/val): 0.0411/0.0325. Time elapsed: 827.84\n",
      "Epoch [382/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 829.94\n",
      "Epoch [383/600], Loss (last training batch/val): 0.0402/0.0325. Time elapsed: 832.10\n",
      "Epoch [384/600], Loss (last training batch/val): 0.0411/0.0325. Time elapsed: 834.23\n",
      "Epoch   385: reducing learning rate of group 0 to 2.4414e-06.\n",
      "Epoch [385/600], Loss (last training batch/val): 0.0413/0.0325. Time elapsed: 836.33\n",
      "Epoch [386/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 838.43\n",
      "Epoch [387/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 840.54\n",
      "Epoch [388/600], Loss (last training batch/val): 0.0407/0.0325. Time elapsed: 842.65\n",
      "Epoch [389/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 844.78\n",
      "Epoch [390/600], Loss (last training batch/val): 0.0407/0.0325. Time elapsed: 846.89\n",
      "Epoch [391/600], Loss (last training batch/val): 0.0389/0.0325. Time elapsed: 849.00\n",
      "Epoch [392/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 851.12\n",
      "Epoch   393: reducing learning rate of group 0 to 1.2207e-06.\n",
      "Epoch [393/600], Loss (last training batch/val): 0.0405/0.0325. Time elapsed: 853.29\n",
      "Epoch [394/600], Loss (last training batch/val): 0.0401/0.0325. Time elapsed: 855.40\n",
      "Epoch [395/600], Loss (last training batch/val): 0.0411/0.0325. Time elapsed: 857.51\n",
      "Epoch [396/600], Loss (last training batch/val): 0.0422/0.0325. Time elapsed: 859.64\n",
      "Epoch [397/600], Loss (last training batch/val): 0.0411/0.0325. Time elapsed: 861.74\n",
      "Epoch [398/600], Loss (last training batch/val): 0.0425/0.0325. Time elapsed: 863.86\n",
      "Epoch [399/600], Loss (last training batch/val): 0.0403/0.0325. Time elapsed: 865.96\n",
      "Epoch [400/600], Loss (last training batch/val): 0.0401/0.0325. Time elapsed: 868.06\n",
      "Epoch   401: reducing learning rate of group 0 to 6.1035e-07.\n",
      "Epoch [401/600], Loss (last training batch/val): 0.0403/0.0325. Time elapsed: 870.19\n",
      "Epoch [402/600], Loss (last training batch/val): 0.0400/0.0325. Time elapsed: 872.33\n",
      "Epoch [403/600], Loss (last training batch/val): 0.0416/0.0325. Time elapsed: 874.41\n",
      "Epoch [404/600], Loss (last training batch/val): 0.0412/0.0325. Time elapsed: 876.52\n",
      "Epoch [405/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 878.61\n",
      "Epoch [406/600], Loss (last training batch/val): 0.0405/0.0325. Time elapsed: 880.72\n",
      "Epoch [407/600], Loss (last training batch/val): 0.0407/0.0325. Time elapsed: 882.86\n",
      "Epoch [408/600], Loss (last training batch/val): 0.0416/0.0325. Time elapsed: 884.99\n",
      "Epoch   409: reducing learning rate of group 0 to 3.0518e-07.\n",
      "Epoch [409/600], Loss (last training batch/val): 0.0398/0.0325. Time elapsed: 887.08\n",
      "Epoch [410/600], Loss (last training batch/val): 0.0390/0.0325. Time elapsed: 889.23\n",
      "Epoch [411/600], Loss (last training batch/val): 0.0394/0.0325. Time elapsed: 891.35\n",
      "Epoch [412/600], Loss (last training batch/val): 0.0395/0.0325. Time elapsed: 893.49\n",
      "Epoch [413/600], Loss (last training batch/val): 0.0398/0.0325. Time elapsed: 895.62\n",
      "Epoch [414/600], Loss (last training batch/val): 0.0410/0.0325. Time elapsed: 898.41\n",
      "Epoch [415/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 900.52\n",
      "Epoch [416/600], Loss (last training batch/val): 0.0407/0.0325. Time elapsed: 902.64\n",
      "Epoch   417: reducing learning rate of group 0 to 1.5259e-07.\n",
      "Epoch [417/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 904.75\n",
      "Epoch [418/600], Loss (last training batch/val): 0.0413/0.0325. Time elapsed: 906.85\n",
      "Epoch [419/600], Loss (last training batch/val): 0.0407/0.0325. Time elapsed: 908.95\n",
      "Epoch [420/600], Loss (last training batch/val): 0.0394/0.0325. Time elapsed: 911.08\n",
      "Epoch [421/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 913.24\n",
      "Epoch [422/600], Loss (last training batch/val): 0.0412/0.0325. Time elapsed: 915.36\n",
      "Epoch [423/600], Loss (last training batch/val): 0.0401/0.0325. Time elapsed: 917.49\n",
      "Epoch [424/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 919.63\n",
      "Epoch   425: reducing learning rate of group 0 to 7.6294e-08.\n",
      "Epoch [425/600], Loss (last training batch/val): 0.0402/0.0325. Time elapsed: 921.73\n",
      "Epoch [426/600], Loss (last training batch/val): 0.0422/0.0325. Time elapsed: 923.84\n",
      "Epoch [427/600], Loss (last training batch/val): 0.0423/0.0325. Time elapsed: 925.95\n",
      "Epoch [428/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 928.06\n",
      "Epoch [429/600], Loss (last training batch/val): 0.0401/0.0325. Time elapsed: 930.15\n",
      "Epoch [430/600], Loss (last training batch/val): 0.0415/0.0325. Time elapsed: 932.27\n",
      "Epoch [431/600], Loss (last training batch/val): 0.0420/0.0325. Time elapsed: 934.39\n",
      "Epoch [432/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 936.51\n",
      "Epoch   433: reducing learning rate of group 0 to 3.8147e-08.\n",
      "Epoch [433/600], Loss (last training batch/val): 0.0400/0.0325. Time elapsed: 938.61\n",
      "Epoch [434/600], Loss (last training batch/val): 0.0399/0.0325. Time elapsed: 940.74\n",
      "Epoch [435/600], Loss (last training batch/val): 0.0415/0.0325. Time elapsed: 942.87\n",
      "Epoch [436/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 945.00\n",
      "Epoch [437/600], Loss (last training batch/val): 0.0419/0.0325. Time elapsed: 947.12\n",
      "Epoch [438/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 949.40\n",
      "Epoch [439/600], Loss (last training batch/val): 0.0411/0.0325. Time elapsed: 951.70\n",
      "Epoch [440/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 953.95\n",
      "Epoch   441: reducing learning rate of group 0 to 1.9073e-08.\n",
      "Epoch [441/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 956.13\n",
      "Epoch [442/600], Loss (last training batch/val): 0.0410/0.0325. Time elapsed: 958.31\n",
      "Epoch [443/600], Loss (last training batch/val): 0.0398/0.0325. Time elapsed: 961.00\n",
      "Epoch [444/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 963.12\n",
      "Epoch [445/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 965.29\n",
      "Epoch [446/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 967.44\n",
      "Epoch [447/600], Loss (last training batch/val): 0.0403/0.0325. Time elapsed: 969.73\n",
      "Epoch [448/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 971.90\n",
      "Epoch [449/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 974.05\n",
      "Epoch [450/600], Loss (last training batch/val): 0.0376/0.0325. Time elapsed: 976.36\n",
      "Epoch [451/600], Loss (last training batch/val): 0.0399/0.0325. Time elapsed: 978.69\n",
      "Epoch [452/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 980.95\n",
      "Epoch [453/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 983.14\n",
      "Epoch [454/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 985.40\n",
      "Epoch [455/600], Loss (last training batch/val): 0.0410/0.0325. Time elapsed: 987.72\n",
      "Epoch [456/600], Loss (last training batch/val): 0.0417/0.0325. Time elapsed: 989.96\n",
      "Epoch [457/600], Loss (last training batch/val): 0.0410/0.0325. Time elapsed: 992.11\n",
      "Epoch [458/600], Loss (last training batch/val): 0.0413/0.0325. Time elapsed: 994.30\n",
      "Epoch [459/600], Loss (last training batch/val): 0.0407/0.0325. Time elapsed: 996.47\n",
      "Epoch [460/600], Loss (last training batch/val): 0.0417/0.0325. Time elapsed: 998.66\n",
      "Epoch [461/600], Loss (last training batch/val): 0.0394/0.0325. Time elapsed: 1000.85\n",
      "Epoch [462/600], Loss (last training batch/val): 0.0393/0.0325. Time elapsed: 1003.09\n",
      "Epoch [463/600], Loss (last training batch/val): 0.0391/0.0325. Time elapsed: 1005.47\n",
      "Epoch [464/600], Loss (last training batch/val): 0.0411/0.0325. Time elapsed: 1007.68\n",
      "Epoch [465/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 1009.86\n",
      "Epoch [466/600], Loss (last training batch/val): 0.0407/0.0325. Time elapsed: 1011.99\n",
      "Epoch [467/600], Loss (last training batch/val): 0.0421/0.0325. Time elapsed: 1014.15\n",
      "Epoch [468/600], Loss (last training batch/val): 0.0392/0.0325. Time elapsed: 1016.31\n",
      "Epoch [469/600], Loss (last training batch/val): 0.0414/0.0325. Time elapsed: 1018.44\n",
      "Epoch [470/600], Loss (last training batch/val): 0.0390/0.0325. Time elapsed: 1020.55\n",
      "Epoch [471/600], Loss (last training batch/val): 0.0411/0.0325. Time elapsed: 1022.68\n",
      "Epoch [472/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 1024.81\n",
      "Epoch [473/600], Loss (last training batch/val): 0.0428/0.0325. Time elapsed: 1026.95\n",
      "Epoch [474/600], Loss (last training batch/val): 0.0410/0.0325. Time elapsed: 1029.08\n",
      "Epoch [475/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 1031.21\n",
      "Epoch [476/600], Loss (last training batch/val): 0.0421/0.0325. Time elapsed: 1033.33\n",
      "Epoch [477/600], Loss (last training batch/val): 0.0399/0.0325. Time elapsed: 1035.46\n",
      "Epoch [478/600], Loss (last training batch/val): 0.0403/0.0325. Time elapsed: 1037.59\n",
      "Epoch [479/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 1039.72\n",
      "Epoch [480/600], Loss (last training batch/val): 0.0401/0.0325. Time elapsed: 1041.85\n",
      "Epoch [481/600], Loss (last training batch/val): 0.0405/0.0325. Time elapsed: 1043.98\n",
      "Epoch [482/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 1046.08\n",
      "Epoch [483/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 1048.18\n",
      "Epoch [484/600], Loss (last training batch/val): 0.0398/0.0325. Time elapsed: 1050.32\n",
      "Epoch [485/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 1052.73\n",
      "Epoch [486/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 1054.83\n",
      "Epoch [487/600], Loss (last training batch/val): 0.0424/0.0325. Time elapsed: 1056.94\n",
      "Epoch [488/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 1059.15\n",
      "Epoch [489/600], Loss (last training batch/val): 0.0412/0.0325. Time elapsed: 1061.36\n",
      "Epoch [490/600], Loss (last training batch/val): 0.0400/0.0325. Time elapsed: 1063.49\n",
      "Epoch [491/600], Loss (last training batch/val): 0.0403/0.0325. Time elapsed: 1065.62\n",
      "Epoch [492/600], Loss (last training batch/val): 0.0402/0.0325. Time elapsed: 1067.73\n",
      "Epoch [493/600], Loss (last training batch/val): 0.0412/0.0325. Time elapsed: 1069.84\n",
      "Epoch [494/600], Loss (last training batch/val): 0.0414/0.0325. Time elapsed: 1071.95\n",
      "Epoch [495/600], Loss (last training batch/val): 0.0412/0.0325. Time elapsed: 1074.08\n",
      "Epoch [496/600], Loss (last training batch/val): 0.0414/0.0325. Time elapsed: 1076.23\n",
      "Epoch [497/600], Loss (last training batch/val): 0.0415/0.0325. Time elapsed: 1078.34\n",
      "Epoch [498/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 1080.49\n",
      "Epoch [499/600], Loss (last training batch/val): 0.0405/0.0325. Time elapsed: 1082.62\n",
      "Epoch [500/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 1084.74\n",
      "Epoch [501/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 1086.85\n",
      "Epoch [502/600], Loss (last training batch/val): 0.0400/0.0325. Time elapsed: 1088.96\n",
      "Epoch [503/600], Loss (last training batch/val): 0.0407/0.0325. Time elapsed: 1091.08\n",
      "Epoch [504/600], Loss (last training batch/val): 0.0420/0.0325. Time elapsed: 1093.20\n",
      "Epoch [505/600], Loss (last training batch/val): 0.0402/0.0325. Time elapsed: 1095.34\n",
      "Epoch [506/600], Loss (last training batch/val): 0.0398/0.0325. Time elapsed: 1097.45\n",
      "Epoch [507/600], Loss (last training batch/val): 0.0418/0.0325. Time elapsed: 1099.55\n",
      "Epoch [508/600], Loss (last training batch/val): 0.0410/0.0325. Time elapsed: 1101.64\n",
      "Epoch [509/600], Loss (last training batch/val): 0.0401/0.0325. Time elapsed: 1103.75\n",
      "Epoch [510/600], Loss (last training batch/val): 0.0418/0.0325. Time elapsed: 1105.85\n",
      "Epoch [511/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 1107.99\n",
      "Epoch [512/600], Loss (last training batch/val): 0.0401/0.0325. Time elapsed: 1110.11\n",
      "Epoch [513/600], Loss (last training batch/val): 0.0392/0.0325. Time elapsed: 1112.22\n",
      "Epoch [514/600], Loss (last training batch/val): 0.0405/0.0325. Time elapsed: 1114.35\n",
      "Epoch [515/600], Loss (last training batch/val): 0.0398/0.0325. Time elapsed: 1116.49\n",
      "Epoch [516/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 1118.58\n",
      "Epoch [517/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 1120.68\n",
      "Epoch [518/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 1122.79\n",
      "Epoch [519/600], Loss (last training batch/val): 0.0392/0.0325. Time elapsed: 1124.87\n",
      "Epoch [520/600], Loss (last training batch/val): 0.0405/0.0325. Time elapsed: 1126.99\n",
      "Epoch [521/600], Loss (last training batch/val): 0.0403/0.0325. Time elapsed: 1129.08\n",
      "Epoch [522/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 1131.20\n",
      "Epoch [523/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 1133.34\n",
      "Epoch [524/600], Loss (last training batch/val): 0.0393/0.0325. Time elapsed: 1135.49\n",
      "Epoch [525/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 1137.61\n",
      "Epoch [526/600], Loss (last training batch/val): 0.0417/0.0325. Time elapsed: 1139.77\n",
      "Epoch [527/600], Loss (last training batch/val): 0.0416/0.0325. Time elapsed: 1141.89\n",
      "Epoch [528/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 1143.99\n",
      "Epoch [529/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 1146.14\n",
      "Epoch [530/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 1148.24\n",
      "Epoch [531/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 1150.37\n",
      "Epoch [532/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 1152.49\n",
      "Epoch [533/600], Loss (last training batch/val): 0.0416/0.0325. Time elapsed: 1154.59\n",
      "Epoch [534/600], Loss (last training batch/val): 0.0390/0.0325. Time elapsed: 1156.74\n",
      "Epoch [535/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 1158.86\n",
      "Epoch [536/600], Loss (last training batch/val): 0.0416/0.0325. Time elapsed: 1160.96\n",
      "Epoch [537/600], Loss (last training batch/val): 0.0413/0.0325. Time elapsed: 1163.08\n",
      "Epoch [538/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 1165.19\n",
      "Epoch [539/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 1167.36\n",
      "Epoch [540/600], Loss (last training batch/val): 0.0414/0.0325. Time elapsed: 1169.45\n",
      "Epoch [541/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 1171.58\n",
      "Epoch [542/600], Loss (last training batch/val): 0.0403/0.0325. Time elapsed: 1173.70\n",
      "Epoch [543/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 1175.79\n",
      "Epoch [544/600], Loss (last training batch/val): 0.0401/0.0325. Time elapsed: 1177.91\n",
      "Epoch [545/600], Loss (last training batch/val): 0.0403/0.0325. Time elapsed: 1180.02\n",
      "Epoch [546/600], Loss (last training batch/val): 0.0416/0.0325. Time elapsed: 1182.11\n",
      "Epoch [547/600], Loss (last training batch/val): 0.0413/0.0325. Time elapsed: 1184.25\n",
      "Epoch [548/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 1186.41\n",
      "Epoch [549/600], Loss (last training batch/val): 0.0402/0.0325. Time elapsed: 1188.56\n",
      "Epoch [550/600], Loss (last training batch/val): 0.0419/0.0325. Time elapsed: 1190.65\n",
      "Epoch [551/600], Loss (last training batch/val): 0.0402/0.0325. Time elapsed: 1192.76\n",
      "Epoch [552/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 1194.89\n",
      "Epoch [553/600], Loss (last training batch/val): 0.0420/0.0325. Time elapsed: 1197.01\n",
      "Epoch [554/600], Loss (last training batch/val): 0.0422/0.0325. Time elapsed: 1199.16\n",
      "Epoch [555/600], Loss (last training batch/val): 0.0391/0.0325. Time elapsed: 1201.28\n",
      "Epoch [556/600], Loss (last training batch/val): 0.0427/0.0325. Time elapsed: 1203.37\n",
      "Epoch [557/600], Loss (last training batch/val): 0.0407/0.0325. Time elapsed: 1205.47\n",
      "Epoch [558/600], Loss (last training batch/val): 0.0408/0.0325. Time elapsed: 1207.58\n",
      "Epoch [559/600], Loss (last training batch/val): 0.0407/0.0325. Time elapsed: 1209.72\n",
      "Epoch [560/600], Loss (last training batch/val): 0.0409/0.0325. Time elapsed: 1211.81\n",
      "Epoch [561/600], Loss (last training batch/val): 0.0397/0.0325. Time elapsed: 1213.91\n",
      "Epoch [562/600], Loss (last training batch/val): 0.0399/0.0325. Time elapsed: 1216.04\n",
      "Epoch [563/600], Loss (last training batch/val): 0.0403/0.0325. Time elapsed: 1218.18\n",
      "Epoch [564/600], Loss (last training batch/val): 0.0416/0.0325. Time elapsed: 1220.29\n",
      "Epoch [565/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 1222.40\n",
      "Epoch [566/600], Loss (last training batch/val): 0.0400/0.0325. Time elapsed: 1224.53\n",
      "Epoch [567/600], Loss (last training batch/val): 0.0422/0.0325. Time elapsed: 1226.61\n",
      "Epoch [568/600], Loss (last training batch/val): 0.0386/0.0325. Time elapsed: 1228.76\n",
      "Epoch [569/600], Loss (last training batch/val): 0.0390/0.0325. Time elapsed: 1230.88\n",
      "Epoch [570/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 1233.03\n",
      "Epoch [571/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 1235.14\n",
      "Epoch [572/600], Loss (last training batch/val): 0.0410/0.0325. Time elapsed: 1237.25\n",
      "Epoch [573/600], Loss (last training batch/val): 0.0400/0.0325. Time elapsed: 1239.34\n",
      "Epoch [574/600], Loss (last training batch/val): 0.0400/0.0325. Time elapsed: 1241.46\n",
      "Epoch [575/600], Loss (last training batch/val): 0.0412/0.0325. Time elapsed: 1243.58\n",
      "Epoch [576/600], Loss (last training batch/val): 0.0402/0.0325. Time elapsed: 1245.74\n",
      "Epoch [577/600], Loss (last training batch/val): 0.0389/0.0325. Time elapsed: 1247.87\n",
      "Epoch [578/600], Loss (last training batch/val): 0.0394/0.0325. Time elapsed: 1249.98\n",
      "Epoch [579/600], Loss (last training batch/val): 0.0390/0.0325. Time elapsed: 1252.11\n",
      "Epoch [580/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 1254.24\n",
      "Epoch [581/600], Loss (last training batch/val): 0.0406/0.0325. Time elapsed: 1256.35\n",
      "Epoch [582/600], Loss (last training batch/val): 0.0415/0.0325. Time elapsed: 1258.48\n",
      "Epoch [583/600], Loss (last training batch/val): 0.0411/0.0325. Time elapsed: 1260.61\n",
      "Epoch [584/600], Loss (last training batch/val): 0.0404/0.0325. Time elapsed: 1262.74\n",
      "Epoch [585/600], Loss (last training batch/val): 0.0399/0.0325. Time elapsed: 1264.86\n",
      "Epoch [586/600], Loss (last training batch/val): 0.0395/0.0325. Time elapsed: 1266.96\n",
      "Epoch [587/600], Loss (last training batch/val): 0.0391/0.0325. Time elapsed: 1269.11\n",
      "Epoch [588/600], Loss (last training batch/val): 0.0395/0.0325. Time elapsed: 1271.24\n",
      "Epoch [589/600], Loss (last training batch/val): 0.0415/0.0325. Time elapsed: 1273.43\n",
      "Epoch [590/600], Loss (last training batch/val): 0.0396/0.0325. Time elapsed: 1275.53\n",
      "Epoch [591/600], Loss (last training batch/val): 0.0423/0.0325. Time elapsed: 1277.66\n",
      "Epoch [592/600], Loss (last training batch/val): 0.0426/0.0325. Time elapsed: 1279.83\n",
      "Epoch [593/600], Loss (last training batch/val): 0.0395/0.0325. Time elapsed: 1282.00\n",
      "Epoch [594/600], Loss (last training batch/val): 0.0414/0.0325. Time elapsed: 1284.12\n",
      "Epoch [595/600], Loss (last training batch/val): 0.0399/0.0325. Time elapsed: 1286.27\n",
      "Epoch [596/600], Loss (last training batch/val): 0.0414/0.0325. Time elapsed: 1288.40\n",
      "Epoch [597/600], Loss (last training batch/val): 0.0390/0.0325. Time elapsed: 1290.55\n",
      "Epoch [598/600], Loss (last training batch/val): 0.0382/0.0325. Time elapsed: 1292.68\n",
      "Epoch [599/600], Loss (last training batch/val): 0.0420/0.0325. Time elapsed: 1294.81\n",
      "Epoch [600/600], Loss (last training batch/val): 0.0399/0.0325. Time elapsed: 1296.95\n"
     ]
    }
   ],
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T09:40:54.395Z",
     "iopub.execute_input": "2020-04-16T09:40:54.411Z",
     "iopub.status.idle": "2020-04-16T10:02:31.330Z",
     "shell.execute_reply": "2020-04-16T10:02:31.365Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning stats"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "adata = pd.DataFrame(\n",
    "    {\n",
    "        \"LR\": learning_rates[len(train_dl)-1::len(train_dl)],\n",
    "        \"Training\":losses[len(train_dl)-1::len(train_dl)], \n",
    "        \"Validation\":val_losses\n",
    "    }\n",
    ").reset_index()\n",
    "alt.Chart(adata, width=900, height=600).transform_fold([\"Training\", \"Validation\"]).mark_line().encode(\n",
    "    x=alt.X(\"index:Q\", axis=alt.Axis(title='Epoch')),\n",
    "    y=alt.Y(\"value:Q\", title=\"Loss\"),\n",
    "    color='key:N'\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 11,
     "data": {
      "application/vnd.vegalite.v4+json": {
       "config": {
        "view": {
         "continuousWidth": 400,
         "continuousHeight": 300
        }
       },
       "data": {
        "url": "http://localhost:15247/31602e0d3023319fa66a59243496d623.json"
       },
       "mark": "line",
       "encoding": {
        "color": {
         "type": "nominal",
         "field": "key"
        },
        "x": {
         "type": "quantitative",
         "axis": {
          "title": "Epoch"
         },
         "field": "index"
        },
        "y": {
         "type": "quantitative",
         "field": "value",
         "title": "Loss"
        }
       },
       "height": 600,
       "transform": [
        {
         "fold": [
          "Training",
          "Validation"
         ]
        }
       ],
       "width": 900,
       "$schema": "https://vega.github.io/schema/vega-lite/v4.0.2.json"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:07:14.198Z",
     "iopub.execute_input": "2020-04-16T10:07:14.209Z",
     "iopub.status.idle": "2020-04-16T10:07:14.227Z",
     "shell.execute_reply": "2020-04-16T10:07:14.263Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), '../models/NewTry.dct')"
   ],
   "outputs": [],
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:07:22.281Z",
     "iopub.execute_input": "2020-04-16T10:07:22.290Z",
     "iopub.status.idle": "2020-04-16T10:07:22.303Z",
     "shell.execute_reply": "2020-04-16T10:07:22.313Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import metrics"
   ],
   "outputs": [],
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:07:25.886Z",
     "iopub.execute_input": "2020-04-16T10:07:25.894Z",
     "iopub.status.idle": "2020-04-16T10:07:25.908Z",
     "shell.execute_reply": "2020-04-16T10:07:25.915Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with completely different data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# data\n",
    "target_tree = phy.read(data_path+'tree/Other_eukaryota_2018.tre', 'newick')\n",
    "test_dataset = gd.Trees(\n",
    "    data_path+\"tree/Eukaryota/\",\n",
    "    data_path+\"alns/other_eukaryota_2018/\",\n",
    "    target_tree\n",
    ")\n",
    "test_dataset.data.x = test_dataset.data.x.float()\n",
    "test_dataset.data.edge_attr = (torch.max(test_dataset.data.edge_attr)+0.001 - test_dataset.data.edge_attr).float()\n",
    "test_dataset.data.edge_index = test_dataset.data.edge_index.long()\n",
    "test_dl = DataLoader(\n",
    "    test_dataset, batch_size=test_batch, num_workers=2\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:07:28.289Z",
     "iopub.execute_input": "2020-04-16T10:07:28.297Z",
     "iopub.status.idle": "2020-04-16T10:07:28.503Z",
     "shell.execute_reply": "2020-04-16T10:07:28.486Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "np.concatenate(nplist).shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 31,
     "data": {
      "text/plain": [
       "(271930, 2)"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "outputExpanded": false,
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "execution": {
     "iopub.status.busy": "2020-04-16T10:12:22.888Z",
     "iopub.execute_input": "2020-04-16T10:12:22.896Z",
     "iopub.status.idle": "2020-04-16T10:12:22.910Z",
     "shell.execute_reply": "2020-04-16T10:12:22.916Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "nplist = list()\n",
    "for data in test_dl:\n",
    "    with torch.no_grad():\n",
    "        data.x, data.edge_index, data.edge_attr = data.x.float(), data.edge_index.long(), data.edge_attr.float()  # CLEAN IT!!!!!\n",
    "        out = model(data.to(torch.device('cuda')))\n",
    "        nplist.append(np.concatenate([data.y.cpu().numpy(), out.detach().cpu().numpy()], axis=1))"
   ],
   "outputs": [],
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:10:08.483Z",
     "iopub.execute_input": "2020-04-16T10:10:08.493Z",
     "shell.execute_reply": "2020-04-16T10:10:09.451Z",
     "iopub.status.idle": "2020-04-16T10:10:09.460Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_results = pd.DataFrame(np.concatenate(nplist), columns=['actual', 'predicted'])"
   ],
   "outputs": [],
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:12:30.995Z",
     "iopub.execute_input": "2020-04-16T10:12:31.004Z",
     "iopub.status.idle": "2020-04-16T10:12:31.017Z",
     "shell.execute_reply": "2020-04-16T10:12:31.024Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "alt.Chart(test_results, width=900, height=600).mark_boxplot(clip=True, outliers=False).encode(\n",
    "    x=alt.X('actual', scale=alt.Scale(domain=(0, 1))),\n",
    "    y=alt.Y('predicted', scale=alt.Scale(domain=(0, 2)))\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 39,
     "data": {
      "application/vnd.vegalite.v4+json": {
       "config": {
        "view": {
         "continuousWidth": 400,
         "continuousHeight": 300
        }
       },
       "data": {
        "url": "http://localhost:15247/3015377d927f33bfc1cb0ef1a109580e.json"
       },
       "mark": {
        "type": "boxplot",
        "clip": true,
        "outliers": false
       },
       "encoding": {
        "x": {
         "type": "quantitative",
         "field": "actual",
         "scale": {
          "domain": [
           0,
           1
          ]
         }
        },
        "y": {
         "type": "quantitative",
         "field": "predicted",
         "scale": {
          "domain": [
           0,
           2
          ]
         }
        }
       },
       "height": 600,
       "width": 900,
       "$schema": "https://vega.github.io/schema/vega-lite/v4.0.2.json"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:17:01.294Z",
     "iopub.execute_input": "2020-04-16T10:17:01.304Z",
     "iopub.status.idle": "2020-04-16T10:17:01.441Z",
     "shell.execute_reply": "2020-04-16T10:17:01.507Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Further assessment"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# first half of the nodes\n",
    "nplist = list()\n",
    "for data in test_dataset:\n",
    "    with torch.no_grad():\n",
    "        data.x, data.edge_index, data.edge_attr = data.x.float(), data.edge_index.long(), data.edge_attr.float()  # CLEAN IT!!!!!\n",
    "        out = model(data.to(torch.device('cuda')))[:60]\n",
    "        nplist.append(np.concatenate([data.y.cpu()[:60].numpy(), out.detach().cpu().numpy()], axis=1))\n",
    "test_results = pd.DataFrame(np.concatenate(nplist, 0), columns=['actual', 'predicted'])\n",
    "test_results.describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 41,
     "data": {
      "text/plain": [
       "             actual      predicted\n",
       "count  1.378800e+05  137880.000000\n",
       "mean   4.496009e-01       0.454507\n",
       "std    4.964652e-01       0.438970\n",
       "min    1.925930e-34       0.000000\n",
       "25%    1.525879e-05       0.000000\n",
       "50%    9.765625e-04       0.370812\n",
       "75%    1.000000e+00       0.888189\n",
       "max    1.000000e+00       2.197329"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.378800e+05</td>\n",
       "      <td>137880.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.496009e-01</td>\n",
       "      <td>0.454507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.964652e-01</td>\n",
       "      <td>0.438970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.925930e-34</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.525879e-05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.765625e-04</td>\n",
       "      <td>0.370812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.888189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.197329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:18:12.984Z",
     "iopub.execute_input": "2020-04-16T10:18:12.994Z",
     "iopub.status.idle": "2020-04-16T10:18:19.547Z",
     "shell.execute_reply": "2020-04-16T10:18:19.762Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(test_results[\"actual\"]==1, test_results[\"predicted\"])\n",
    "roc_data = pd.DataFrame({\"True Positive Rate\": tpr, \"False Positive Rate\": fpr})"
   ],
   "outputs": [],
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:18:20.624Z",
     "iopub.execute_input": "2020-04-16T10:18:20.631Z",
     "iopub.status.idle": "2020-04-16T10:18:20.643Z",
     "shell.execute_reply": "2020-04-16T10:18:20.652Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "alt.Chart(roc_data, width=900, height=600).mark_line().encode(\n",
    "    x=\"False Positive Rate\",\n",
    "    y=\"True Positive Rate\"\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 43,
     "data": {
      "application/vnd.vegalite.v4+json": {
       "config": {
        "view": {
         "continuousWidth": 400,
         "continuousHeight": 300
        }
       },
       "data": {
        "url": "http://localhost:15247/d2d628529742bcd3c628b3a071ccef49.json"
       },
       "mark": "line",
       "encoding": {
        "x": {
         "type": "quantitative",
         "field": "False Positive Rate"
        },
        "y": {
         "type": "quantitative",
         "field": "True Positive Rate"
        }
       },
       "height": 600,
       "width": 900,
       "$schema": "https://vega.github.io/schema/vega-lite/v4.0.2.json"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:18:25.142Z",
     "iopub.execute_input": "2020-04-16T10:18:25.148Z",
     "iopub.status.idle": "2020-04-16T10:18:25.164Z",
     "shell.execute_reply": "2020-04-16T10:18:25.191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# second half\n",
    "nplist = list()\n",
    "for data in test_dataset:\n",
    "    with torch.no_grad():\n",
    "        data.x, data.edge_index, data.edge_attr = data.x.float(), data.edge_index.long(), data.edge_attr.float()  # CLEAN IT!!!!!\n",
    "        out = model(data.to(torch.device('cuda')))\n",
    "        nplist.append(np.concatenate([data.y.cpu().numpy(), out.detach().cpu().numpy()], axis=1))\n",
    "test_results = pd.DataFrame(np.concatenate(nplist, 0), columns=['actual', 'predicted'])\n",
    "test_results.describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 44,
     "data": {
      "text/plain": [
       "             actual      predicted\n",
       "count  2.719300e+05  271930.000000\n",
       "mean   5.098314e-01       0.513201\n",
       "std    4.988907e-01       0.437022\n",
       "min    1.925930e-34       0.000000\n",
       "25%    6.103516e-05       0.000000\n",
       "50%    1.000000e+00       0.509334\n",
       "75%    1.000000e+00       0.898723\n",
       "max    1.000000e+00       2.197329"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.719300e+05</td>\n",
       "      <td>271930.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.098314e-01</td>\n",
       "      <td>0.513201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.988907e-01</td>\n",
       "      <td>0.437022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.925930e-34</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.103516e-05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.509334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.898723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.197329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:18:30.442Z",
     "iopub.execute_input": "2020-04-16T10:18:30.450Z",
     "iopub.status.idle": "2020-04-16T10:18:37.112Z",
     "shell.execute_reply": "2020-04-16T10:18:37.138Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prec, rec, threshold = metrics.precision_recall_curve(test_results[\"actual\"]==1, test_results[\"predicted\"])\n",
    "roc_data = pd.DataFrame({\"Precision\": prec, \"Recall\": rec})"
   ],
   "outputs": [],
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:18:40.199Z",
     "iopub.execute_input": "2020-04-16T10:18:40.210Z",
     "iopub.status.idle": "2020-04-16T10:18:40.223Z",
     "shell.execute_reply": "2020-04-16T10:18:40.239Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "alt.Chart(roc_data, width=900, height=600).mark_line().encode(\n",
    "    x=\"Recall\",\n",
    "    y=\"Precision\"\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 46,
     "data": {
      "application/vnd.vegalite.v4+json": {
       "config": {
        "view": {
         "continuousWidth": 400,
         "continuousHeight": 300
        }
       },
       "data": {
        "url": "http://localhost:15247/99042ee0059f613df0c40683c79b67bf.json"
       },
       "mark": "line",
       "encoding": {
        "x": {
         "type": "quantitative",
         "field": "Recall"
        },
        "y": {
         "type": "quantitative",
         "field": "Precision"
        }
       },
       "height": 600,
       "width": 900,
       "$schema": "https://vega.github.io/schema/vega-lite/v4.0.2.json"
      },
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:18:43.586Z",
     "iopub.execute_input": "2020-04-16T10:18:43.594Z",
     "iopub.status.idle": "2020-04-16T10:18:43.693Z",
     "shell.execute_reply": "2020-04-16T10:18:43.728Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_results.describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 47,
     "data": {
      "text/plain": [
       "             actual      predicted\n",
       "count  2.719300e+05  271930.000000\n",
       "mean   5.098314e-01       0.513201\n",
       "std    4.988907e-01       0.437022\n",
       "min    1.925930e-34       0.000000\n",
       "25%    6.103516e-05       0.000000\n",
       "50%    1.000000e+00       0.509334\n",
       "75%    1.000000e+00       0.898723\n",
       "max    1.000000e+00       2.197329"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.719300e+05</td>\n",
       "      <td>271930.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.098314e-01</td>\n",
       "      <td>0.513201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.988907e-01</td>\n",
       "      <td>0.437022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.925930e-34</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.103516e-05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.509334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.898723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.197329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-04-16T10:19:42.923Z",
     "iopub.execute_input": "2020-04-16T10:19:42.931Z",
     "iopub.status.idle": "2020-04-16T10:19:42.950Z",
     "shell.execute_reply": "2020-04-16T10:19:42.978Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "metrics.confusion_matrix(test_results[\"actual\"]==1, test_results[\"predicted\"]>0.628)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 48,
     "data": {
      "text/plain": [
       "array([[133463,    128],\n",
       "       [  8434, 129905]])"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "execution": {
     "iopub.status.busy": "2020-04-16T10:19:48.109Z",
     "iopub.execute_input": "2020-04-16T10:19:48.118Z",
     "iopub.status.idle": "2020-04-16T10:19:48.220Z",
     "shell.execute_reply": "2020-04-16T10:19:48.242Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(len(nplist)):\n",
    "    print(np.average(-np.log2(nplist[i])[:,0][-np.log2(nplist[i])[:,0]>0]))"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "execution": {
     "iopub.status.busy": "2020-03-05T10:10:01.274Z",
     "iopub.execute_input": "2020-03-05T10:10:01.281Z",
     "iopub.status.idle": "2020-03-05T10:10:01.298Z",
     "shell.execute_reply": "2020-03-05T10:10:01.306Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "y = 1\n",
    "max(0, -y*(1-0)*100)"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "execution": {
     "iopub.status.busy": "2020-03-05T18:54:31.130Z",
     "iopub.execute_input": "2020-03-05T18:54:31.139Z",
     "iopub.status.idle": "2020-03-05T18:54:31.154Z",
     "shell.execute_reply": "2020-03-05T18:54:31.161Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "source_hidden": false,
     "outputs_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}